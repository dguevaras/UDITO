#!/usr/bin/env python3
"""
Gemma 2B LLM Integration para UDI
Proporciona capacidades de generaciÃ³n de lenguaje natural
"""

import os
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GemmaLLM")

class GemmaLLM:
    def __init__(self, model_name: str = "google/gemma-2b-it"):
        """Inicializa el LLM Gemma 2B"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.is_initialized = False
        
        # Cargar token de Hugging Face
        self.hf_token = self._load_hf_token()
        
        logger.info(f"Inicializando Gemma LLM: {model_name} en {self.device}")
        self._load_model()
    
    def _load_hf_token(self) -> Optional[str]:
        """Carga el token de Hugging Face desde archivo"""
        try:
            token_file = Path("config/huggingface_token.txt")
            if token_file.exists():
                with open(token_file, 'r') as f:
                    token = f.read().strip()
                logger.info("âœ… Token de Hugging Face cargado desde archivo")
                return token
            else:
                logger.warning("âš ï¸ Archivo de token no encontrado, usando variable de entorno")
                return os.environ.get('HUGGING_FACE_HUB_TOKEN')
        except Exception as e:
            logger.error(f"Error al cargar token: {e}")
            return os.environ.get('HUGGING_FACE_HUB_TOKEN')
    
    def _load_model(self):
        """Carga el modelo y tokenizer de Gemma"""
        try:
            logger.info("Cargando tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left",
                token=self.hf_token
            )
            
            # Configurar padding token si no existe
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info("Cargando modelo...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                token=self.hf_token
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            self.is_initialized = True
            logger.info("Modelo Gemma 2B cargado exitosamente")
            
        except Exception as e:
            logger.error(f"Error al cargar modelo Gemma: {e}")
            raise
    
    def generate_response(self, query: str, context: str, max_length: int = 256) -> str:
        """Genera una respuesta usando Gemma 2B optimizado para velocidad"""
        try:
            if not self.is_initialized:
                raise ValueError("Modelo no inicializado")
            
            # OPTIMIZACIÃ“N: Cache de respuestas frecuentes
            cache_key = f"{query[:50]}_{hash(context[:100])}"
            if hasattr(self, 'response_cache') and cache_key in self.response_cache:
                logger.info("âœ… Respuesta obtenida desde cache")
                return self.response_cache[cache_key]
            
            # Construir prompt optimizado para velocidad
            prompt = self._build_optimized_prompt(query, context)
            
            # Tokenizar entrada optimizada
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,  # Reducido para velocidad
                padding=True
            )
            
            if self.device == "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # OPTIMIZACIÃ“N: ParÃ¡metros de generaciÃ³n para velocidad
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,  # Reducido para velocidad
                    temperature=0.6,  # MÃ¡s determinÃ­stico
                    do_sample=True,
                    top_p=0.8,  # Reducido para velocidad
                    repetition_penalty=1.05,  # Reducido
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_beams=1,  # Sin beam search para velocidad
                    early_stopping=True  # Parar cuando sea necesario
                )
            
            # Decodificar respuesta
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()
            
            # OPTIMIZACIÃ“N: Cachear respuesta
            if not hasattr(self, 'response_cache'):
                self.response_cache = {}
            self.response_cache[cache_key] = response
            
            logger.info(f"âš¡ Respuesta generada rÃ¡pidamente: {len(response)} caracteres")
            return response
            
        except Exception as e:
            logger.error(f"Error al generar respuesta optimizada: {e}")
            return "Lo siento, tuve un problema al generar la respuesta."
    
    def _build_optimized_prompt(self, query: str, context: str) -> str:
        """Construye prompt optimizado para velocidad"""
        # OPTIMIZACIÃ“N: Prompt mÃ¡s corto y directo
        prompt = f"""<start_of_turn>user
Eres UDI, asistente universitario. Responde de forma natural y concisa.

DOCUMENTOS: {context[:500]}  # Limitar contexto

PREGUNTA: {query}

Respuesta breve y Ãºtil:<end_of_turn>
<start_of_turn>model
"""
        return prompt
    
    def get_model_info(self) -> dict:
        """Obtiene informaciÃ³n del modelo"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "is_initialized": self.is_initialized,
            "parameters": sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            "model_type": "Gemma 2B Instruction Tuned"
        }

def create_gemma_llm(model_name: str = "google/gemma-2b-it", device: str = None) -> GemmaLLM:
    """FunciÃ³n de conveniencia para crear una instancia de GemmaLLM"""
    return GemmaLLM(model_name, device)

if __name__ == "__main__":
    # Prueba del mÃ³dulo
    print("ğŸ§ª Probando mÃ³dulo Gemma LLM...")
    
    try:
        llm = GemmaLLM()
        info = llm.get_model_info()
        print(f"âœ… Modelo cargado: {info}")
        
        # Prueba de generaciÃ³n
        test_query = "Â¿CuÃ¡les son los horarios de la universidad?"
        test_context = "La universidad UDIT abre de lunes a viernes de 8:00 a 18:00 horas. La secretarÃ­a atiende en el mismo horario."
        
        response = llm.generate_response(test_query, test_context)
        print(f"ğŸ“ Respuesta de prueba: {response}")
        
    except Exception as e:
        print(f"âŒ Error en prueba: {e}")
