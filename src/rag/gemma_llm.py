#!/usr/bin/env python3
"""
Gemma 2B LLM Integration para UDIT
Proporciona capacidades de generaci√≥n de lenguaje natural
"""

import os
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GemmaLLM")

class GemmaLLM:
    def __init__(self, model_name: str = "google/gemma-2b-it"):
        """Inicializa el LLM Gemma 2B"""
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.is_initialized = False
        
        # Cargar token de Hugging Face
        self.hf_token = self._load_hf_token()
        
        logger.info(f"Inicializando Gemma LLM: {model_name} en {self.device}")
        self._load_model()
    
    def _load_hf_token(self) -> Optional[str]:
        """Carga el token de Hugging Face desde archivo"""
        try:
            token_file = Path("config/huggingface_token.txt")
            if token_file.exists():
                with open(token_file, 'r') as f:
                    token = f.read().strip()
                logger.info("‚úÖ Token de Hugging Face cargado desde archivo")
                return token
            else:
                logger.warning("‚ö†Ô∏è Archivo de token no encontrado, usando variable de entorno")
                return os.environ.get('HUGGING_FACE_HUB_TOKEN')
        except Exception as e:
            logger.error(f"Error al cargar token: {e}")
            return os.environ.get('HUGGING_FACE_HUB_TOKEN')
    
    def _load_model(self):
        """Carga el modelo y tokenizer de Gemma"""
        try:
            logger.info("Cargando tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left",
                token=self.hf_token
            )
            
            # Configurar padding token si no existe
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info("Cargando modelo...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                token=self.hf_token
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            self.is_initialized = True
            logger.info("Modelo Gemma 2B cargado exitosamente")
            
        except Exception as e:
            logger.error(f"Error al cargar modelo Gemma: {e}")
            raise
    
    def generate_response(self, query: str, context: str, max_length: int = 256) -> str:
        """Genera una respuesta usando Gemma 2B optimizado para velocidad"""
        try:
            if not self.is_initialized:
                raise ValueError("Modelo no inicializado")
            
            # OPTIMIZACI√ìN: Cache de respuestas frecuentes
            cache_key = f"{query[:50]}_{hash(context[:100])}"
            if hasattr(self, 'response_cache') and cache_key in self.response_cache:
                logger.info("‚úÖ Respuesta obtenida desde cache")
                return self.response_cache[cache_key]
            
            # Construir prompt optimizado para velocidad
            prompt = self._build_optimized_prompt(query, context)
            
            # Tokenizar entrada optimizada
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,  # Reducido para velocidad
                padding=True
            )
            
            if self.device == "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # OPTIMIZACI√ìN: Par√°metros de generaci√≥n para velocidad
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,  # Reducido para velocidad
                    temperature=0.6,  # M√°s determin√≠stico
                    do_sample=True,
                    top_p=0.8,  # Reducido para velocidad
                    repetition_penalty=1.05,  # Reducido
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_beams=1,  # Sin beam search para velocidad
                    early_stopping=True  # Parar cuando sea necesario
                )
            
            # Decodificar SOLO los tokens nuevos para evitar truncados/corrupci√≥n
            try:
                input_len = inputs["input_ids"].shape[1]
                generated_ids = outputs[0][input_len:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
            except Exception:
                # Fallback seguro
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
            
            # OPTIMIZACI√ìN: Cachear respuesta
            if not hasattr(self, 'response_cache'):
                self.response_cache = {}
            self.response_cache[cache_key] = response
            
            logger.info(f"‚ö° Respuesta generada r√°pidamente: {len(response)} caracteres")
            return response
            
        except Exception as e:
            logger.error(f"Error al generar respuesta optimizada: {e}")
            return "Lo siento, tuve un problema al generar la respuesta."
    
    def _build_optimized_prompt(self, query: str, context: str) -> str:
        """Construye prompt optimizado para velocidad"""
        # Instrucciones: responder SOLO con base en DOCUMENTOS. Si no est√°, indicarlo expl√≠citamente.
        safe_context = context[:1200]
        prompt = (
            f"<start_of_turn>user\n"
            f"Eres UDI, asistente universitario. Responde de forma natural, concisa (1‚Äì3 l√≠neas) y bas√°ndote \n"
            f"exclusivamente en la informaci√≥n de DOCUMENTOS. Si la informaci√≥n no est√° en DOCUMENTOS, responde \n"
            f"exactamente: 'No encontrado en documentos.' No inventes horarios ni normas que no aparezcan.\n\n"
            f"DOCUMENTOS:\n{safe_context}\n\n"
            f"PREGUNTA: {query}\n\n"
            f"Respuesta:\n<end_of_turn>\n<start_of_turn>model\n"
        )
        return prompt
    
    def get_model_info(self) -> dict:
        """Obtiene informaci√≥n del modelo"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "is_initialized": self.is_initialized,
            "parameters": sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            "model_type": "Gemma 2B Instruction Tuned"
        }

def create_gemma_llm(model_name: str = "google/gemma-2b-it", device: str = None) -> GemmaLLM:
    """Funci√≥n de conveniencia para crear una instancia de GemmaLLM"""
    return GemmaLLM(model_name, device)

if __name__ == "__main__":
    # Prueba del m√≥dulo
    print("üß™ Probando m√≥dulo Gemma LLM...")
    
    try:
        llm = GemmaLLM()
        info = llm.get_model_info()
        print(f"‚úÖ Modelo cargado: {info}")
        
        # Prueba de generaci√≥n
        test_query = "¬øCu√°les son los horarios de la universidad?"
        test_context = "La universidad UDIT abre de lunes a viernes de 8:00 a 18:00 horas. La secretar√≠a atiende en el mismo horario."
        
        response = llm.generate_response(test_query, test_context)
        print(f"üìù Respuesta de prueba: {response}")
        
    except Exception as e:
        print(f"‚ùå Error en prueba: {e}")
